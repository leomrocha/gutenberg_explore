Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11780/11780.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11780/11780.zip with Exception [E088] Text of length 176429755 exceeds maximum of 100000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11781/11781.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11781/11781.zip with Exception [E088] Text of length 162361739 exceeds maximum of 100000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/2/2/2/2220/2220.zip
Saving to /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/2/2/2/2220/2220.stats.json.gz
Saving to /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/2/2/2/2220/2220.stats_all.json.gz
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/2/2/2/2221/2221.zip
Saving to /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/2/2/2/2221/2221.stats.json.gz
Saving to /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/2/2/2/2221/2221.stats_all.json.gz
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/2/2/2/2222/2222.zip
Saving to /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/2/2/2/2222/2222.stats.json.gz
Saving to /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/2/2/2/2222/2222.stats_all.json.gz
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/2/2/2/2223/2223.zip
Saving to /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/2/2/2/2223/2223.stats.json.gz
Saving to /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/2/2/2/2223/2223.stats_all.json.gz
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11782/11782.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11782/11782.zip with Exception [E088] Text of length 150764385 exceeds maximum of 100000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11783/11783.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11783/11783.zip with Exception [E088] Text of length 141921174 exceeds maximum of 100000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11784/11784.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11784/11784.zip with Exception [E088] Text of length 138802633 exceeds maximum of 100000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11785/11785.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11785/11785.zip with Exception [E088] Text of length 137172622 exceeds maximum of 100000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11786/11786.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11786/11786.zip with Exception [E088] Text of length 134719955 exceeds maximum of 100000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11787/11787.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11787/11787.zip with Exception [E088] Text of length 115497252 exceeds maximum of 100000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11788/11788.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11788/11788.zip with Exception [E088] Text of length 107417449 exceeds maximum of 100000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11789/11789.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11789/11789.zip with Exception [E088] Text of length 103417295 exceeds maximum of 100000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11790/11790.ziphecking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11789/11789.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/8/11789/11789.zip with Exception [E088] Text of length 103417295 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11790/11790.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11790/11790.zip with Exception [E088] Text of length 91842779 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11791/11791.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11791/11791.zip with Exception [E088] Text of length 86097701 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11792/11792.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11792/11792.zip with Exception [E088] Text of length 77641812 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11793/11793.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11793/11793.zip with Exception [E088] Text of length 65182450 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11794/11794.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11794/11794.zip with Exception [E088] Text of length 65016714 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11795/11795.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11795/11795.zip with Exception [E088] Text of length 47915627 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11796/11796.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11796/11796.zip with Exception [E088] Text of length 50384920 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11797/11797.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11797/11797.zip with Exception [E088] Text of length 160237889 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11798/11798.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/7/9/11798/11798.zip with Exception [E088] Text of length 51292294 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/8/0/11800/11800-8.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/1/1/8/0/11800/11800-8.zip with Exception [E088] Text of length 31105911 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3501/3501.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3501/3501.zip with Exception [E088] Text of length 281540932 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3502/3502.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3502/3502.zip with Exception [E088] Text of length 252968278 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3503/3503.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3503/3503.zip with Exception [E088] Text of length 223376861 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3504/3504.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3504/3504.zip with Exception [E088] Text of length 204178715 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3505/3505.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3505/3505.zip with Exception [E088] Text of length 201332590 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3506/3506.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3506/3506.zip with Exception [E088] Text of length 184951922 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3507/3507.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3507/3507.zip with Exception [E088] Text of length 169572452 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3508/3508.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3508/3508.zip with Exception [E088] Text of length 147833098 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3509/3509.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/0/3509/3509.zip with Exception [E088] Text of length 144088597 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/1/3510/3510.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/1/3510/3510.zip with Exception [E088] Text of length 146165798 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/1/3511/3511.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/1/3511/3511.zip with Exception [E088] Text of length 150458345 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/1/3513/3513.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/1/3513/3513.zip with Exception [E088] Text of length 119561040 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/1/3514/3514.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/1/3514/3514.zip with Exception [E088] Text of length 107837112 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.
Processing /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/1/3515/3515.zip
Error processing file /home/leo/projects/Datasets/text/Gutenberg/Gutenberg/aleph.gutenberg.org/3/5/1/3515/3515.zip with Exception [E088] Text of length 84723720 exceeds maximum of 30000000.0. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.